"""
OpenAlex MCP Server ç»¼åˆæµ‹è¯•è„šæœ¬

ç›´æ¥é€šè¿‡ FastMCP Client æµ‹è¯• MCP Server çš„æ‰€æœ‰å·¥å…·:
1. search_openalex - ç®€å•æœç´¢
2. query_openalex - é«˜çº§æŸ¥è¯¢
3. fetch_openalex - è·å–è¯¦æƒ…
4. sample_openalex - éšæœºé‡‡æ ·
5. batch_fetch_openalex - æ‰¹é‡è·å–
6. autocomplete_openalex - è‡ªåŠ¨è¡¥å…¨
7. ngrams_openalex - N-grams åˆ†æ

è¿è¡Œ: uv run python test_server.py
"""

import asyncio
from fastmcp import Client
from fastmcp.exceptions import ToolError


async def test_autocomplete_two_step_workflow(client: Client):
    """
    æµ‹è¯• 1: ä¸¤æ­¥æŸ¥è¯¢æ¨¡å¼
    å…ˆç”¨ autocomplete è·å– IDï¼Œå†ç”¨ query æŸ¥è¯¢
    """
    print("=" * 70)
    print("æµ‹è¯• 1: ä¸¤æ­¥æŸ¥è¯¢æ¨¡å¼ (autocomplete â†’ query)")
    print("       ç›®æ ‡: æŸ¥æ‰¾æ–¯å¦ç¦å¤§å­¦ 2024 å¹´çš„ AI è®ºæ–‡")
    print("=" * 70)
    
    # Step 1: autocomplete è·å–æœºæ„ ID
    result = await client.call_tool("autocomplete_openalex", {
        "query": "Stanford University",
        "entity_type": "institutions"
    })
    
    stanford_id = result.data["results"][0]["id"].split("/")[-1]
    stanford_name = result.data["results"][0]["display_name"]
    print(f"Step 1 - Autocomplete: {stanford_name} â†’ {stanford_id}")
    
    # Step 2: ä½¿ç”¨ ID æŸ¥è¯¢è®ºæ–‡
    result = await client.call_tool("query_openalex", {
        "entity_type": "works",
        "filter": f"authorships.institutions.id:{stanford_id},publication_year:2024",
        "search": "artificial intelligence",
        "sort": "cited_by_count:desc",
        "select": "id,title,cited_by_count,publication_year",
        "limit": 5
    })
    
    print(f"Step 2 - Query: æ‰¾åˆ° {result.data['meta']['count']:,} ç¯‡è®ºæ–‡")
    print("\nå‰ 5 ç¯‡é«˜å¼•è®ºæ–‡:")
    for i, w in enumerate(result.data["results"], 1):
        title = w["title"][:55] + "..." if len(w["title"]) > 55 else w["title"]
        print(f"  {i}. {title}")
        print(f"     å¼•ç”¨: {w['cited_by_count']}")
    print()
    return True


async def test_complex_filter_combination(client: Client):
    """
    æµ‹è¯• 2: å¤æ‚è¿‡æ»¤å™¨ç»„åˆ
    å¤šæ¡ä»¶ AND ç»„åˆ + OR æ“ä½œ + èŒƒå›´ç­›é€‰
    """
    print("=" * 70)
    print("æµ‹è¯• 2: å¤æ‚è¿‡æ»¤å™¨ç»„åˆ")
    print("       ç›®æ ‡: ä¸­å›½æˆ–ç¾å›½ä½œè€… 2023-2024 å¹´å‘è¡¨çš„ LLM é«˜å¼•è®ºæ–‡")
    print("=" * 70)
    
    result = await client.call_tool("query_openalex", {
        "entity_type": "works",
        "filter": "authorships.countries:CN|US,publication_year:2023-2024,cited_by_count:>50",
        "search": "large language model",
        "sort": "cited_by_count:desc",
        "select": "id,title,cited_by_count,publication_year,authorships",
        "limit": 5
    })
    
    print(f"Filter: authorships.countries:CN|US,publication_year:2023-2024,cited_by_count:>50")
    print(f"æ€»æ•°: {result.data['meta']['count']:,} ç¯‡")
    print("\nå‰ 5 ç¯‡:")
    for i, w in enumerate(result.data["results"], 1):
        countries = set()
        for a in w.get("authorships", []):
            countries.update(a.get("countries", []))
        title = w["title"][:50] + "..." if len(w["title"]) > 50 else w["title"]
        print(f"  {i}. {title}")
        print(f"     å¼•ç”¨: {w['cited_by_count']} | å¹´ä»½: {w['publication_year']} | å›½å®¶: {', '.join(countries)}")
    print()
    return True


async def test_group_by_statistics(client: Client):
    """
    æµ‹è¯• 3: åˆ†ç»„èšåˆç»Ÿè®¡
    ä½¿ç”¨ group_by è¿›è¡Œå¤šç»´åº¦ç»Ÿè®¡åˆ†æ
    """
    print("=" * 70)
    print("æµ‹è¯• 3: åˆ†ç»„èšåˆç»Ÿè®¡ (group_by)")
    print("       ç›®æ ‡: åˆ†æ Nature æœŸåˆŠè¿‘ 10 å¹´çš„å‘æ–‡é‡è¶‹åŠ¿")
    print("=" * 70)
    
    # å…ˆè·å– Nature çš„ ID
    autocomplete = await client.call_tool("autocomplete_openalex", {
        "query": "Nature",
        "entity_type": "sources"
    })
    nature_id = autocomplete.data["results"][0]["id"].split("/")[-1]
    print(f"Nature ID: {nature_id}")
    
    # æŒ‰å¹´ä»½åˆ†ç»„
    result = await client.call_tool("query_openalex", {
        "entity_type": "works",
        "filter": f"primary_location.source.id:{nature_id},publication_year:2015-2024",
        "group_by": "publication_year"
    })
    
    print(f"\nNature æœŸåˆŠå‘æ–‡é‡ç»Ÿè®¡ (2015-2024):")
    groups = sorted(result.data.get("group_by", []), key=lambda x: x.get("key", ""), reverse=True)
    max_count = max(g.get("count", 0) for g in groups) if groups else 1
    for g in groups:
        year = g.get("key", "Unknown")
        count = g.get("count", 0)
        bar_len = int(count / max_count * 30)
        bar = "â–ˆ" * bar_len
        print(f"  {year}: {count:>5} {bar}")
    print()
    return True


async def test_batch_fetch_efficiency(client: Client):
    """
    æµ‹è¯• 4: æ‰¹é‡è·å–æ•ˆç‡æµ‹è¯•
    ä½¿ç”¨ batch_fetch ä¸€æ¬¡æ€§è·å–å¤šä¸ªå®ä½“
    """
    print("=" * 70)
    print("æµ‹è¯• 4: æ‰¹é‡è·å– (batch_fetch)")
    print("       ç›®æ ‡: æ‰¹é‡è·å– 5 ç¯‡ç»å…¸ AI è®ºæ–‡çš„è¯¦æƒ…")
    print("=" * 70)
    
    # ç»å…¸ AI è®ºæ–‡ DOI
    dois = [
        "10.1038/nature14539",  # Deep learning (LeCun, Bengio, Hinton)
        "10.1162/neco.1997.9.8.1735",  # LSTM
        "10.1145/3065386",  # AlexNet (ImageNet)
        "10.48550/arXiv.1706.03762",  # Attention Is All You Need
        "10.48550/arXiv.1810.04805",  # BERT
    ]
    
    result = await client.call_tool("batch_fetch_openalex", {
        "identifiers": dois,
        "entity_type": "works",
        "select": "id,title,cited_by_count,publication_year,doi"
    })
    
    print(f"æ‰¹é‡è·å– {len(dois)} ç¯‡è®ºæ–‡:")
    for w in result.data.get("results", []):
        title = w["title"][:45] + "..." if len(w["title"]) > 45 else w["title"]
        print(f"  â€¢ {title}")
        print(f"    å¼•ç”¨: {w['cited_by_count']:,} | å¹´ä»½: {w['publication_year']}")
    print()
    return True


async def test_sample_reproducibility(client: Client):
    """
    æµ‹è¯• 5: éšæœºé‡‡æ ·å¯é‡å¤æ€§
    ä½¿ç”¨ seed ç¡®ä¿é‡‡æ ·ç»“æœå¯é‡å¤
    """
    print("=" * 70)
    print("æµ‹è¯• 5: éšæœºé‡‡æ ·å¯é‡å¤æ€§ (sample with seed)")
    print("       ç›®æ ‡: éªŒè¯ç›¸åŒ seed è¿”å›ç›¸åŒç»“æœ")
    print("=" * 70)
    
    # ç¬¬ä¸€æ¬¡é‡‡æ ·
    result1 = await client.call_tool("sample_openalex", {
        "entity_type": "works",
        "sample_size": 5,
        "seed": 42,
        "filter": "publication_year:2024,is_oa:true",
        "select": "id,title"
    })
    
    # ç¬¬äºŒæ¬¡é‡‡æ · (ç›¸åŒ seed)
    result2 = await client.call_tool("sample_openalex", {
        "entity_type": "works",
        "sample_size": 5,
        "seed": 42,
        "filter": "publication_year:2024,is_oa:true",
        "select": "id,title"
    })
    
    ids1 = [w["id"] for w in result1.data["results"]]
    ids2 = [w["id"] for w in result2.data["results"]]
    
    print(f"é‡‡æ · 1 (seed=42): {len(ids1)} ç¯‡è®ºæ–‡")
    print(f"é‡‡æ · 2 (seed=42): {len(ids2)} ç¯‡è®ºæ–‡")
    print(f"ç»“æœä¸€è‡´: {'âœ… æ˜¯' if ids1 == ids2 else 'âŒ å¦'}")
    
    if ids1 == ids2:
        print("\né‡‡æ ·ç»“æœ:")
        for w in result1.data["results"]:
            title = w["title"][:60] + "..." if len(w["title"]) > 60 else w["title"]
            print(f"  â€¢ {title}")
    print()
    return ids1 == ids2


async def test_fetch_with_related_analysis(client: Client):
    """
    æµ‹è¯• 6: è·å–è¯¦æƒ… + å…³è”åˆ†æ
    è·å–è®ºæ–‡è¯¦æƒ…ååˆ†æå…¶å¼•ç”¨ç½‘ç»œ
    """
    print("=" * 70)
    print("æµ‹è¯• 6: è®ºæ–‡è¯¦æƒ… + å¼•ç”¨ç½‘ç»œåˆ†æ")
    print("       ç›®æ ‡: åˆ†æ Deep Learning è®ºæ–‡çš„å½±å“åŠ›")
    print("=" * 70)
    
    # ä½¿ç”¨ Deep Learning (LeCun, Bengio, Hinton) è®ºæ–‡ - Nature 2015
    result = await client.call_tool("fetch_openalex", {
        "identifier": "W2103795898",  # Deep learning paper
        "entity_type": "work"
    })
    
    work = result.data
    
    # æ£€æŸ¥æ˜¯å¦è¿”å›é”™è¯¯
    if work.get("error"):
        print(f"âš ï¸ è®ºæ–‡è·å–å¤±è´¥: {work.get('message')}")
        # å¤‡ç”¨: ä½¿ç”¨æœç´¢è·å–
        search_result = await client.call_tool("search_openalex", {
            "query": "Deep learning LeCun Bengio Hinton",
            "entity_type": "works",
            "limit": 1
        })
        if search_result.data["results"]:
            work = search_result.data["results"][0]
        else:
            print("å¤‡ç”¨æœç´¢ä¹Ÿå¤±è´¥")
            return False
    
    print(f"æ ‡é¢˜: {work['title']}")
    print(f"å¼•ç”¨æ•°: {work['cited_by_count']:,}")
    print(f"å¹´ä»½: {work['publication_year']}")
    
    # åˆ†æä½œè€…
    authorships = work.get("authorships", [])
    if authorships:
        print(f"\nä½œè€… ({len(authorships)} ä½):")
        for a in authorships[:5]:
            name = a["author"]["display_name"]
            insts = [i["display_name"] for i in a.get("institutions", [])]
            inst_str = f" @ {insts[0]}" if insts else ""
            print(f"  â€¢ {name}{inst_str}")
    
    # åˆ†æå¼•ç”¨è¯¥è®ºæ–‡çš„é«˜å¼•è®ºæ–‡
    work_id = work['id'].split('/')[-1]
    citing_result = await client.call_tool("query_openalex", {
        "entity_type": "works",
        "filter": f"cites:{work_id}",
        "sort": "cited_by_count:desc",
        "select": "id,title,cited_by_count,publication_year",
        "limit": 5
    })
    
    print(f"\nå¼•ç”¨è¯¥è®ºæ–‡çš„é«˜å¼•è®ºæ–‡ (å…± {citing_result.data['meta']['count']:,} ç¯‡):")
    for w in citing_result.data["results"]:
        title = w["title"][:50] + "..." if len(w["title"]) > 50 else w["title"]
        print(f"  â€¢ {title}")
        print(f"    å¼•ç”¨: {w['cited_by_count']:,} | å¹´ä»½: {w['publication_year']}")
    print()
    return True


async def test_cross_entity_workflow(client: Client):
    """
    æµ‹è¯• 7: è·¨å®ä½“å·¥ä½œæµ
    ä½œè€… â†’ æœºæ„ â†’ è®ºæ–‡ â†’ èµ„åŠ©æœºæ„ çš„å®Œæ•´é“¾è·¯
    """
    print("=" * 70)
    print("æµ‹è¯• 7: è·¨å®ä½“å·¥ä½œæµ")
    print("       ç›®æ ‡: ä»ä½œè€…å‡ºå‘åˆ†æå…¶ç ”ç©¶è„‰ç»œ")
    print("=" * 70)
    
    # Step 1: æœç´¢ä½œè€…
    result = await client.call_tool("search_openalex", {
        "query": "Yoshua Bengio",
        "entity_type": "authors",
        "sort": "cited_by_count",
        "limit": 1
    })
    
    author = result.data["results"][0]
    author_id = author["id"].split("/")[-1]
    print(f"Step 1 - ä½œè€…: {author['display_name']}")
    print(f"         è®ºæ–‡æ•°: {author['works_count']:,} | æ€»å¼•ç”¨: {author['cited_by_count']:,}")
    
    # Step 2: è·å–ä½œè€…è¯¦æƒ…
    detail = await client.call_tool("fetch_openalex", {
        "identifier": author_id,
        "entity_type": "author"
    })
    
    affiliations = detail.data.get("affiliations", [])
    if affiliations:
        inst_name = affiliations[0].get("institution", {}).get("display_name", "N/A")
        print(f"\nStep 2 - ä¸»è¦æœºæ„: {inst_name}")
    
    # Step 3: è·å–è¯¥ä½œè€…é«˜å¼•è®ºæ–‡
    works = await client.call_tool("query_openalex", {
        "entity_type": "works",
        "filter": f"author.id:{author_id}",
        "sort": "cited_by_count:desc",
        "select": "id,title,cited_by_count,funders",
        "limit": 3
    })
    
    print(f"\nStep 3 - ä»£è¡¨ä½œå“:")
    all_funders = set()
    for w in works.data["results"]:
        title = w["title"][:50] + "..." if len(w["title"]) > 50 else w["title"]
        print(f"  â€¢ {title}")
        print(f"    å¼•ç”¨: {w['cited_by_count']:,}")
        for f in w.get("funders", []):
            all_funders.add(f.get("display_name", "Unknown"))
    
    # Step 4: åˆ†æèµ„åŠ©æœºæ„
    if all_funders:
        print(f"\nStep 4 - èµ„åŠ©æœºæ„:")
        for f in list(all_funders)[:5]:
            print(f"  â€¢ {f}")
    print()
    return True


async def test_tool_error_handling(client: Client):
    """
    æµ‹è¯• 8: é”™è¯¯å¤„ç†
    æµ‹è¯• ToolError å’Œ API é”™è¯¯å¤„ç†
    """
    print("=" * 70)
    print("æµ‹è¯• 8: é”™è¯¯å¤„ç†")
    print("       ç›®æ ‡: éªŒè¯ ToolError å’Œå‚æ•°æ ¡éªŒ")
    print("=" * 70)
    
    # æµ‹è¯•ç©ºåˆ—è¡¨
    try:
        await client.call_tool("batch_fetch_openalex", {
            "identifiers": [],
            "entity_type": "works"
        })
        print("âŒ åº”è¯¥æŠ›å‡ºé”™è¯¯: ç©ºåˆ—è¡¨")
        return False
    except ToolError as e:
        print(f"âœ… ç©ºåˆ—è¡¨ â†’ ToolError: {e}")
    
    # æµ‹è¯•è¶…è¿‡ 50 ä¸ª ID
    try:
        await client.call_tool("batch_fetch_openalex", {
            "identifiers": [f"W{i}" for i in range(60)],
            "entity_type": "works"
        })
        print("âŒ åº”è¯¥æŠ›å‡ºé”™è¯¯: è¶…è¿‡ 50 ä¸ª")
        return False
    except ToolError as e:
        print(f"âœ… è¶…è¿‡é™åˆ¶ â†’ ToolError: {e}")
    
    # æµ‹è¯•æ— æ•ˆ ID (åº”è¿”å›ç©ºç»“æœè€ŒéæŠ¥é”™)
    result = await client.call_tool("fetch_openalex", {
        "identifier": "W999999999999999",
        "entity_type": "work"
    })
    
    if result.data.get("error"):
        print(f"âœ… æ— æ•ˆ ID â†’ é”™è¯¯å“åº”: {result.data['error_type']}")
    else:
        print(f"âœ… æ— æ•ˆ ID æŸ¥è¯¢å®Œæˆ (å¯èƒ½è¿”å›ç©ºæˆ–é”™è¯¯)")
    
    print()
    return True


async def test_ngrams_text_analysis(client: Client):
    """
    æµ‹è¯• 9: N-grams æ–‡æœ¬åˆ†æ
    è·å–è®ºæ–‡çš„è¯é¢‘æ•°æ®
    """
    print("=" * 70)
    print("æµ‹è¯• 9: N-grams æ–‡æœ¬åˆ†æ")
    print("       ç›®æ ‡: åˆ†æè®ºæ–‡å…¨æ–‡çš„å…³é”®è¯é¢‘")
    print("=" * 70)
    
    # ä½¿ç”¨ä¸€ç¯‡æœ‰ ngrams æ•°æ®çš„è®ºæ–‡
    result = await client.call_tool("ngrams_openalex", {
        "work_id": "W2741809807"  # BERT è®ºæ–‡
    })
    
    ngrams = result.data.get("ngrams", [])
    if ngrams:
        print(f"è·å–åˆ° {len(ngrams)} ä¸ª N-grams")
        print("\né«˜é¢‘è¯ (top 10):")
        for ng in ngrams[:10]:
            print(f"  â€¢ {ng['ngram']}: å‡ºç° {ng['ngram_count']} æ¬¡ (tf={ng['term_frequency']:.4f})")
    else:
        print("è¯¥è®ºæ–‡æ—  N-grams æ•°æ® (å¯èƒ½æœªè¢«å…¨æ–‡ç´¢å¼•)")
    
    print()
    return True


async def test_advanced_search_boolean(client: Client):
    """
    æµ‹è¯• 10: é«˜çº§å¸ƒå°”æœç´¢
    ä½¿ç”¨å¤æ‚çš„ Boolean è¯­æ³•æœç´¢
    """
    print("=" * 70)
    print("æµ‹è¯• 10: é«˜çº§å¸ƒå°”æœç´¢")
    print("        ç›®æ ‡: ä½¿ç”¨ AND/OR/NOT è¿›è¡Œç²¾ç¡®æœç´¢")
    print("=" * 70)
    
    # å¤æ‚ Boolean æœç´¢
    result = await client.call_tool("query_openalex", {
        "entity_type": "works",
        "search": '("machine learning" OR "deep learning") AND (healthcare OR medical) NOT review',
        "filter": "publication_year:2024,is_oa:true,language:en",
        "sort": "cited_by_count:desc",
        "select": "id,title,cited_by_count,type",
        "limit": 5
    })
    
    print('Search: ("machine learning" OR "deep learning") AND (healthcare OR medical) NOT review')
    print("Filter: publication_year:2024,is_oa:true,language:en")
    print(f"ç»“æœ: {result.data['meta']['count']:,} ç¯‡")
    
    print("\nå‰ 5 ç¯‡:")
    for i, w in enumerate(result.data["results"], 1):
        title = w["title"][:55] + "..." if len(w["title"]) > 55 else w["title"]
        print(f"  {i}. {title}")
        print(f"     ç±»å‹: {w['type']} | å¼•ç”¨: {w['cited_by_count']}")
    print()
    return True


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n")
    print("â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " OpenAlex MCP Server ç»¼åˆæµ‹è¯• ".center(68) + "â•‘")
    print("â•‘" + " ç›´æ¥é€šè¿‡ FastMCP Client æµ‹è¯•æ‰€æœ‰ 7 ä¸ªå·¥å…· ".center(68) + "â•‘")
    print("â•š" + "â•" * 68 + "â•")
    print()
    
    tests = [
        ("ä¸¤æ­¥æŸ¥è¯¢æ¨¡å¼ (autocomplete â†’ query)", test_autocomplete_two_step_workflow),
        ("å¤æ‚è¿‡æ»¤å™¨ç»„åˆ", test_complex_filter_combination),
        ("åˆ†ç»„èšåˆç»Ÿè®¡", test_group_by_statistics),
        ("æ‰¹é‡è·å–", test_batch_fetch_efficiency),
        ("éšæœºé‡‡æ ·å¯é‡å¤æ€§", test_sample_reproducibility),
        ("è®ºæ–‡è¯¦æƒ… + å¼•ç”¨ç½‘ç»œ", test_fetch_with_related_analysis),
        ("è·¨å®ä½“å·¥ä½œæµ", test_cross_entity_workflow),
        ("é”™è¯¯å¤„ç†", test_tool_error_handling),
        ("N-grams æ–‡æœ¬åˆ†æ", test_ngrams_text_analysis),
        ("é«˜çº§å¸ƒå°”æœç´¢", test_advanced_search_boolean),
    ]
    
    passed = 0
    failed = 0
    
    # é€šè¿‡ FastMCP Client è¿æ¥åˆ° MCP Server
    async with Client("openalex_mcp_server.py") as client:
        print("âœ… MCP Client å·²è¿æ¥\n")
        
        # åˆ—å‡ºå¯ç”¨å·¥å…·
        tools = await client.list_tools()
        print(f"å¯ç”¨å·¥å…· ({len(tools)} ä¸ª):")
        for tool in tools:
            print(f"  â€¢ {tool.name}")
        print()
        
        # è¿è¡Œæµ‹è¯•
        for name, test_func in tests:
            try:
                result = await test_func(client)
                if result:
                    passed += 1
                    print(f"âœ… {name} é€šè¿‡\n")
                else:
                    failed += 1
                    print(f"âŒ {name} å¤±è´¥\n")
                await asyncio.sleep(0.2)  # é¿å…é€Ÿç‡é™åˆ¶
            except Exception as e:
                failed += 1
                print(f"âŒ {name} å¼‚å¸¸: {e}\n")
    
    # æ±‡æ€»
    print("=" * 70)
    print(f"æµ‹è¯•ç»“æœ: {passed} é€šè¿‡, {failed} å¤±è´¥")
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MCP æœåŠ¡å™¨åŠŸèƒ½æ­£å¸¸ã€‚")
    else:
        print(f"âš ï¸  æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(main())
